{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from modules.unet import UNet\n",
    "from modules.percep_nets import ResidualsNetUp, ResidualsNetDown\n",
    "from transfers import Transfer, pretrained_transfers\n",
    "from logger import Logger, VisdomLogger\n",
    "from datasets import load_train_val, load_test, load_ood, load_all, TaskDataset\n",
    "from task_configs import tasks, RealityTask\n",
    "from model_configs import model_types\n",
    "from models import DataParallelModel\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No deletion\n",
      "In (git) scaling-reset\n",
      "Logging to environment YKIVVA_jupyter_visualize_\n"
     ]
    }
   ],
   "source": [
    "logger = VisdomLogger(\"Jupyter_vis\", env=\"YKIVVA_jupyter_visualize_\", port=PORT, server=SERVER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"/scratch/kivva/projects/consistency_LS\"\n",
    "model_class = \"resnet_based\"\n",
    "model_type = model_types[model_class]\n",
    "\n",
    "pipeline = \"multitask\"\n",
    "res_dir = os.path.join(project_dir, model_class, pipeline,)\n",
    "dir_multitask = os.path.join(res_dir, \"results_YKIVVA_resnet3_multitask:x->n&r_v1_/models\")\n",
    "\n",
    "pipeline = \"direct\"\n",
    "res_dir = os.path.join(project_dir, model_class, pipeline,)\n",
    "dir_direct = os.path.join(res_dir, \"results_YKIVVA_resnet3_direct:n->r&r->n_v1_/models\")\n",
    "\n",
    "pipeline = \"consistency\"\n",
    "res_dir = os.path.join(project_dir, model_class, pipeline,)\n",
    "dir_consistency = os.path.join(res_dir, \"results_YKIVVA_resnet3_perceptual:x->n&r_direct_v1_/models\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_down = DataParallelModel(model_type[\"rgb\"][\"down\"][0]())\n",
    "normal_down = DataParallelModel(model_type[\"normal\"][\"down\"][0]())\n",
    "depth_down = DataParallelModel(model_type[\"depth_zbuffer\"][\"down\"][0]())\n",
    "normal_up = DataParallelModel(model_type[\"normal\"][\"up\"][0]())\n",
    "depth_up = DataParallelModel(model_type[\"depth_zbuffer\"][\"up\"][0]())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "path = dir_multitask+\"/rgb2LS.pth\"\n",
    "if os.path.isfile(path):\n",
    "    rgb_down.load_weights(path)\n",
    "    print(1)\n",
    "\n",
    "path = dir_direct + \"/normal2LS.pth\"\n",
    "if os.path.isfile(path):\n",
    "    normal_down.load_weights(path)\n",
    "    print(2)\n",
    "\n",
    "path = dir_direct+\"/depth_zbuffer2LS.pth\"\n",
    "if os.path.isfile(path):\n",
    "    depth_down.load_weights(path)\n",
    "    print(3)\n",
    "\n",
    "path = dir_multitask+\"/LS2normal.pth\"\n",
    "if os.path.isfile(path):\n",
    "    normal_up.load_weights(path)\n",
    "    print(4)\n",
    "\n",
    "path = dir_multitask+\"/LS2depth_zbuffer.pth\"\n",
    "if os.path.isfile(path):\n",
    "    depth_up.load_weights(path)\n",
    "    print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2depth, path = pretrained_transfers[('rgb', 'depth_zbuffer')]\n",
    "rgb2depth = DataParallelModel.load(rgb2depth(), path)\n",
    "\n",
    "rgb2normal, path = pretrained_transfers[('rgb', 'normal')]\n",
    "rgb2normal = DataParallelModel.load(rgb2normal(), path)\n",
    "\n",
    "normal2depth, path = pretrained_transfers[('normal', 'depth_zbuffer')]\n",
    "normal2depth = DataParallelModel.load(normal2depth(), path)\n",
    "\n",
    "depth2normal, path = pretrained_transfers[('depth_zbuffer', 'normal')]\n",
    "depth2normal = DataParallelModel.load(depth2normal(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCURACY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rgb file len: 8785\n",
      "    Intersection files len:  8785\n",
      "tensor(5.5808, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "src_task = tasks.rgb\n",
    "target_task = tasks.depth_zbuffer\n",
    "tasks_set = [src_task, target_task]\n",
    "\n",
    "buildings = ['almena']\n",
    "batch_size = 64\n",
    "test_loader = load_all(tasks_set, buildings, batch_size=batch_size)\n",
    "\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "accuracy = 0\n",
    "for in_data, out_data in test_iter:\n",
    "    in_data = in_data.to(DEVICE)\n",
    "    out_data = out_data.to(DEVICE)\n",
    "    if src_task==tasks.rgb:\n",
    "        model_down = rgb_down.to(DEVICE)\n",
    "        if target_task==tasks.normal:\n",
    "            model = rgb2normal.to(DEVICE)\n",
    "        if target_task==tasks.depth_zbuffer:\n",
    "            model = rgb2depth.to(DEVICE)\n",
    "    if src_task==tasks.normal:\n",
    "        model_down = normal_down.to(DEVICE)\n",
    "        model = normal2depth.to(DEVICE)\n",
    "    if src_task==tasks.depth_zbuffer:\n",
    "        model_down = depth_down.to(DEVICE)\n",
    "        model = depth2normal.to(DEVICE)\n",
    "    \n",
    "    if target_task==tasks.normal:\n",
    "        model_up = normal_up.to(DEVICE)\n",
    "    if target_task==tasks.depth_zbuffer:\n",
    "        model_up = depth_up.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         out = model(in_data)\n",
    "        out = model_up(model_down(in_data))\n",
    "    accuracy_tmp, _ = target_task.norm(out, out_data, compute_mse=False, batch_mean=False)\n",
    "    accuracy += accuracy_tmp.sum()\n",
    "\n",
    "print(accuracy / 8785. * 100.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERCEPTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rgb file len: 8785\n",
      "    Intersection files len:  8785\n",
      "tensor(17.3254, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "src_task = tasks.rgb\n",
    "middle_task = tasks.depth_zbuffer\n",
    "target_task = tasks.normal\n",
    "tasks_set = [src_task, target_task]\n",
    "\n",
    "buildings = ['almena']\n",
    "batch_size = 64\n",
    "test_loader = load_all(tasks_set, buildings, batch_size=batch_size)\n",
    "\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "accuracy = 0\n",
    "for in_data, out_data in test_iter:\n",
    "    in_data = in_data.to(DEVICE)\n",
    "    out_data = out_data.to(DEVICE)\n",
    "    if src_task==tasks.rgb:\n",
    "        model_down = rgb_down.to(DEVICE)\n",
    "    if src_task==tasks.normal:\n",
    "        model_down = normal_down.to(DEVICE)\n",
    "    if src_task==tasks.depth_zbuffer:\n",
    "        model_down = depth_down.to(DEVICE)\n",
    "    \n",
    "    if middle_task==tasks.normal:\n",
    "        model_up = normal_up.to(DEVICE)\n",
    "        model_down_per = normal_down.to(DEVICE)\n",
    "        model_up_per = depth_up.to(DEVICE)\n",
    "        model_per = normal2depth.to(DEVICE)\n",
    "    if middle_task==tasks.depth_zbuffer:\n",
    "        model_up = depth_up.to(DEVICE)\n",
    "        model_down_per = depth_down.to(DEVICE)\n",
    "        model_up_per = normal_up.to(DEVICE)\n",
    "        model_per = depth2normal.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model_per(model_up(model_down(in_data)))\n",
    "#         out = model_up_per(model_down_per(model_up(model_down(in_data))))\n",
    "    accuracy_tmp, _ = target_task.norm(out, out_data, compute_mse=False, batch_mean=False)\n",
    "    accuracy += accuracy_tmp.sum()\n",
    "\n",
    "print(accuracy / 8785. * 100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    depth_zbuffer file len: 8785\n",
      "    Intersection files len:  8785\n"
     ]
    }
   ],
   "source": [
    "# buildings = ['almena']\n",
    "# batch_size = 16\n",
    "# test_loader = load_all([tasks.depth_zbuffer, tasks.normal], buildings, batch_size=batch_size)\n",
    "\n",
    "# test_iter = iter(test_loader)\n",
    "# in_data, out_data = next(test_iter)\n",
    "# out = model_per(in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = list(out_data.shape)\n",
    "shape[1] = 3\n",
    "out = out.clamp(min=0, max=1).expand(*shape)\n",
    "out_data = out_data.clamp(min=0, max=1).expand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [out_data, out]\n",
    "\n",
    "logger.images_grouped(images, f\"results:r, r(n(x))\", resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_set = [tasks.rgb, tasks.normal, tasks.depth_zbuffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = load_test(tasks_set, buildings=['almena', 'albertville', 'espanola'])\n",
    "# test = RealityTask.from_static(\n",
    "#     \"test\", test_set, tasks_set\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rgb file len: 8785\n",
      "    Intersection files len:  8785\n"
     ]
    }
   ],
   "source": [
    "buildings = ['almena']\n",
    "batch_size = 64\n",
    "test_loader = load_all(tasks_set, buildings, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_iter = iter(test_loader)\n",
    "#rgb_data, normal_data, depth_data = next(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_data = test.task_data[tasks.rgb].to(DEVICE)\n",
    "normal_data = test.task_data[tasks.normal].to(DEVICE)\n",
    "depth_data = test.task_data[tasks.depth_zbuffer].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rgb_out = rgb_down(rgb_data)\n",
    "    normal_out = normal_up(rgb_out)\n",
    "    depth_out = depth_up(rgb_out)\n",
    "    \n",
    "    shape = list(rgb_data.shape)\n",
    "    shape[1] = 3\n",
    "    normal_out = normal_out.clamp(min=0, max=1).expand(*shape)\n",
    "    depth_out = depth_out.clamp(min=0, max=1).expand(*shape)\n",
    "    depth_data = depth_data.clamp(min=0, max=1).expand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [depth_data, depth_out, normal_data, normal_out]\n",
    "\n",
    "logger.images_grouped(images, f\"results:r, r(x), n, n(x)\", resize=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
