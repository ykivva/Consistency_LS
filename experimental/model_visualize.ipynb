{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from modules.unet import UNet\n",
    "from modules.percep_nets import ResidualsNetUp, ResidualsNetDown\n",
    "from transfers import Transfer, pretrained_transfers\n",
    "from logger import Logger, VisdomLogger\n",
    "from datasets import load_train_val, load_test, load_ood\n",
    "from task_configs import tasks, RealityTask\n",
    "from model_configs import model_types\n",
    "from models import DataParallelModel\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA = False\n",
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No deletion\n",
      "In (git) scaling-reset\n",
      "Logging to environment visualize_models\n"
     ]
    }
   ],
   "source": [
    "logger = VisdomLogger(\"visualize\", env=\"visualize_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kivva/Consistency_LS'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR_MODELS\n",
    "model_class = \"resnet_based\"\n",
    "model_type = model_types[model_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_down = DataParallelModel(model_type[\"rgb\"][\"down\"][0]())\n",
    "normal_down = DataParallelModel(model_type[\"normal\"][\"down\"][0]())\n",
    "depth_down = DataParallelModel(model_type[\"depth_zbuffer\"][\"down\"][0]())\n",
    "normal_up = DataParallelModel(model_type[\"normal\"][\"up\"][0]())\n",
    "depth_up = DataParallelModel(model_type[\"depth_zbuffer\"][\"up\"][0]())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_down.load_weights(MODELS_DIR+\"/rgb2LS.pth\")\n",
    "normal_down.load_weights(RESULTS_DIR_MODELS+\"/normal2LS.pth\")\n",
    "depth_down.load_weights(RESULTS_DIR_MODELS+\"/depth_zbuffer2LS.pth\")\n",
    "normal_up.load_weights(RESULTS_DIR_MODELS+\"/LS2normal.pth\")\n",
    "depth_up.load_weights(RESULTS_DIR_MODELS+\"/LS2depth_zbuffer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images in almena:\n",
      "    rgb file len: 8785\n",
      "    Intersection files len:  8785\n",
      "number of images in albertville:\n",
      "    rgb file len: 7405\n",
      "    Intersection files len:  7405\n",
      "number of images in espanola:\n",
      "    rgb file len: 2282\n",
      "    Intersection files len:  2282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n",
      "/home/kivva/miniconda/envs/py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:70: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629395347/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n"
     ]
    }
   ],
   "source": [
    "test_set = load_test([tasks.rgb, tasks.normal, tasks.depth_zbuffer],\n",
    "                     buildings=['almena', 'albertville', 'espanola'])\n",
    "test = RealityTask.from_static(\n",
    "    \"test\", test_set, [tasks.rgb, tasks.normal, tasks.depth_zbuffer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_data = test.task_data[tasks.rgb].to(DEVICE)\n",
    "normal_data = test.task_data[tasks.normal].to(DEVICE)\n",
    "depth_data = test.task_data[tasks.depth_zbuffer].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rgb_out = rgb_down(rgb_data)\n",
    "    normal_out = normal_up(rgb_out)\n",
    "    depth_out = depth_up(rgb_out)\n",
    "    \n",
    "    shape = list(rgb_data.shape)\n",
    "    shape[1] = 3\n",
    "    normal_out = normal_out.clamp(min=0, max=1).expand(*shape)\n",
    "    depth_out = depth_out.clamp(min=0, max=1).expand(*shape)\n",
    "    depth_data = depth_data.clamp(min=0, max=1).expand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [depth_data, depth_out, normal_data, normal_out]\n",
    "\n",
    "logger.images_grouped(images, f\"results:r, r(n), *r(n)\", resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTITASK APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTITASK_MODELS_DIR = \"/scratch/kivva/projects/consistency_LS/multitask/results_mae/models\"\n",
    "\n",
    "dest_tasks = [tasks.normal, tasks.depth_zbuffer]\n",
    "src_task = tasks.rgb\n",
    "\n",
    "model_rgb_down = model_types[\"rgb\"][\"down\"][0]()\n",
    "model_normal_up = model_types[\"normal\"][\"up\"][0]()\n",
    "model_depth_up = model_types[\"depth_zbuffer\"][\"up\"][0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rgb_down.set_grads(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = model_rgb_down.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rgb_down = MULTITASK_MODELS_DIR + '/rgb_down.pth'\n",
    "path_normal_up = MULTITASK_MODELS_DIR + \"/normal_up.pth\"\n",
    "path_depth_up = MULTITASK_MODELS_DIR + \"/depth_zbuffer_up.pth\"\n",
    "\n",
    "model_rgb_down.load_weights(path_rgb_down)\n",
    "model_normal_up.load_weights(path_normal_up)\n",
    "model_depth_up.load_weights(path_depth_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal = UNet_LS(model_up=model_normal_up, model_down=model_rgb_down)\n",
    "rgb2depth = UNet_LS(model_up=model_depth_up, model_down=model_rgb_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ood images:  21\n"
     ]
    }
   ],
   "source": [
    "ood_set = load_ood([tasks.rgb])\n",
    "ood = RealityTask.from_static(\"ood\",  ood_set, [tasks.rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No deletion\n",
      "In (git) scaling-reset\n",
      "Logging to environment visualize_models\n"
     ]
    }
   ],
   "source": [
    "logger = VisdomLogger(\"visualize\", env=\"visualize_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ood.task_data[tasks.rgb].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal.to(DEVICE)\n",
    "rgb2depth.to(DEVICE)\n",
    "rgb2normal = nn.DataParallel(rgb2normal) if not isinstance(rgb2normal, nn.DataParallel) else rgb2normal\n",
    "rgb2depth = nn.DataParallel(rgb2depth)if not isinstance(rgb2depth, nn.DataParallel) else rgb2depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_normal = rgb2normal(input_data)\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_depth = rgb2depth(input_data)\n",
    "    \n",
    "    shape = list(out_normal.shape)\n",
    "    shape[1] = 3\n",
    "    out_normal = out_normal.clamp(min=0, max=1).expand(*shape)\n",
    "    out_depth = out_depth.clamp(min=0, max=1).expand(*shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [input_data, out_normal, out_depth]\n",
    "\n",
    "logger.images_grouped(images, f\"multitask_ood:rgb->normal, rgb->depth\", resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODELS_DIR = \"./baseline/models\"\n",
    "\n",
    "dest_tasks = [tasks.normal, tasks.depth_zbuffer]\n",
    "src_task = tasks.rgb\n",
    "\n",
    "baseline_rgb2normal, path_rgb2normal = pretrained_transfers[(src_task.name, dest_tasks[0].name)]\n",
    "baseline_rgb2depth, path_rgb2depth = pretrained_transfers[(src_task.name, dest_tasks[1].name)]\n",
    "path_rgb2normal = BASELINE_MODELS_DIR + \"/unet_baseline_standardval.pth\"\n",
    "path_rgb2depth = BASELINE_MODELS_DIR + \"/rgb2zdepth_buffer.pth\"\n",
    "baseline_rgb2depth = baseline_rgb2depth()\n",
    "baseline_rgb2normal = baseline_rgb2normal()\n",
    "baseline_rgb2depth = DataParallelModel(baseline_rgb2depth)\n",
    "baseline_rgb2normal = DataParallelModel(baseline_rgb2normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rgb2depth.load_weights(path_rgb2depth)\n",
    "baseline_rgb2normal.load_weights(path_rgb2normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rgb2depth.to(DEVICE)\n",
    "baseline_rgb2normal.to(DEVICE)\n",
    "input_data = ood.task_data[tasks.rgb].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out_normal = baseline_rgb2normal(input_data)\n",
    "    input_data.to(DEVICE)\n",
    "    out_depth = baseline_rgb2depth(input_data)\n",
    "    \n",
    "    shape = list(out_normal.shape)\n",
    "    shape[1] = 3\n",
    "    out_normal = out_normal.clamp(min=0, max=1).expand(*shape)\n",
    "    out_depth = out_depth.clamp(min=0, max=1).expand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [input_data, out_normal, out_depth]\n",
    "\n",
    "logger.images_grouped(images, f\"baseline_ood:rgb->normal, rgb->depth\", resize=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZE ANOTHER MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"/home/kivva/Consistency_LD/models\"\n",
    "\n",
    "dest_tasks = [tasks.normal, tasks.depth_zbuffer]\n",
    "src_task = tasks.rgb\n",
    "\n",
    "model1_rgb_down = model_types[\"rgb\"][\"down\"][0]()\n",
    "model1_normal_up = model_types[\"normal\"][\"up\"][0]()\n",
    "model1_depth_up = model_types[\"depth_zbuffer\"][\"up\"][0]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rgb_down = MODELS_DIR + '/rgb_down.pth'\n",
    "path_normal_up = MODELS_DIR + \"/normal_up.pth\"\n",
    "path_depth_up = MODELS_DIR + \"/depth_zbuffer_up.pth\"\n",
    "\n",
    "model1_rgb_down.load_weights(path_rgb_down)\n",
    "model1_normal_up.load_weights(path_normal_up)\n",
    "model1_depth_up.load_weights(path_depth_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal = UNet_LS(model_up=model1_normal_up, model_down=model1_rgb_down)\n",
    "rgb2depth = UNet_LS(model_up=model1_depth_up, model_down=model1_rgb_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = ood.task_data[tasks.rgb].to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb2normal.to(DEVICE)\n",
    "rgb2depth.to(DEVICE)\n",
    "rgb2normal = nn.DataParallel(rgb2normal) if not isinstance(rgb2normal, nn.DataParallel) else rgb2normal\n",
    "rgb2depth = nn.DataParallel(rgb2depth)if not isinstance(rgb2depth, nn.DataParallel) else rgb2depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_normal = rgb2normal(input_data)\n",
    "    input_data = input_data.to(DEVICE)\n",
    "    out_depth = rgb2depth(input_data)\n",
    "    \n",
    "    shape = list(out_normal.shape)\n",
    "    shape[1] = 3\n",
    "    out_normal = out_normal.clamp(min=0, max=1).expand(*shape)\n",
    "    out_depth = out_depth.clamp(min=0, max=1).expand(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [input_data, out_normal, out_depth]\n",
    "\n",
    "logger.images_grouped(images, f\"model_ood:rgb->normal, rgb->depth\", resize=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
